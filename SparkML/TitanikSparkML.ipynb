{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/nfs/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/26 09:31:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkTitanikJob\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-a-2esavchenko:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkTitanikJob</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdb978941c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "titanic_df = spark.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|\n",
      "|       0|     3|  male|30.0|    0|    0| 8.4583|       Q|          0|    1|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|          0|    1|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|          4|    0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|          2|    0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|          1|    0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|          2|    0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|          0|    1|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|          0|    1|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|          6|    0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|          0|    1|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|          0|    1|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|          5|    0|\n",
      "|       1|     2|  male|30.0|    0|    0|   13.0|       S|          0|    1|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|          1|    0|\n",
      "|       1|     3|female|30.0|    0|    0|  7.225|       C|          0|    1|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_index = StringIndexer(inputCol='Sex', outputCol=\"Sex_index\")\n",
    "embarked_index = StringIndexer(inputCol='Embarked', outputCol=\"Embarked_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "titanic_df = sex_index.fit(titanic_df).transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = embarked_index.fit(titanic_df).transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|Sex_index|Embarked_index|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|      0.0|           0.0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|      1.0|           1.0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|      1.0|           0.0|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|      1.0|           0.0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|  male|30.0|    0|    0| 8.4583|       Q|          0|    1|      0.0|           2.0|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|          4|    0|      0.0|           0.0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|          2|    0|      1.0|           0.0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|          1|    0|      1.0|           1.0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|          2|    0|      1.0|           0.0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|          0|    1|      1.0|           0.0|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|          6|    0|      0.0|           0.0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|          0|    1|      1.0|           0.0|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|          0|    1|      1.0|           0.0|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|          5|    0|      0.0|           2.0|\n",
      "|       1|     2|  male|30.0|    0|    0|   13.0|       S|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|          1|    0|      1.0|           0.0|\n",
      "|       1|     3|female|30.0|    0|    0|  7.225|       C|          0|    1|      1.0|           1.0|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Age', 'SibSp', 'SibSp', 'Parch', 'Fare', 'Alone', 'Sex_index', 'Embarked_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "feature_vector= feature.transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|Sex_index|Embarked_index|            features|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|      0.0|           0.0|[3.0,22.0,1.0,1.0...|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|      1.0|           1.0|[1.0,38.0,1.0,1.0...|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|      1.0|           0.0|[3.0,26.0,0.0,0.0...|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|      1.0|           0.0|[1.0,35.0,1.0,1.0...|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|      0.0|           0.0|(9,[0,1,5,6],[3.0...|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = feature_vector.randomSplit([0.8, 0.2],seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|  Fare|Embarked|Family_Size|Alone|Sex_index|Embarked_index|            features|\n",
      "+--------+------+------+----+-----+-----+------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|       0|     1|female| 2.0|    1|    2|151.55|       S|          3|    0|      1.0|           0.0|[1.0,2.0,1.0,1.0,...|\n",
      "|       0|     1|female|25.0|    1|    2|151.55|       S|          3|    0|      1.0|           0.0|[1.0,25.0,1.0,1.0...|\n",
      "|       0|     1|  male|18.0|    1|    0| 108.9|       C|          1|    0|      0.0|           1.0|[1.0,18.0,1.0,1.0...|\n",
      "|       0|     1|  male|19.0|    1|    0|  53.1|       S|          1|    0|      0.0|           0.0|[1.0,19.0,1.0,1.0...|\n",
      "|       0|     1|  male|19.0|    3|    2| 263.0|       S|          5|    0|      0.0|           0.0|[1.0,19.0,3.0,3.0...|\n",
      "+--------+------+------+----+-----+-----+------+--------+-----------+-----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/26 09:42:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/26 09:42:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,50.0,0.0,0.0...|\n",
      "|       1.0|       0|(9,[0,1,4,5],[1.0...|\n",
      "|       1.0|       0|[1.0,24.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "\n",
    "lrModel = lr.fit(training_data)\n",
    "lr_prediction = lrModel.transform(test_data)\n",
    "lr_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression [Accuracy] = 0.813793\n",
      "LogisticRegression [Error] = 0.186207 \n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"LogisticRegression [Accuracy] = %g\"% (lr_accuracy))\n",
    "print(\"LogisticRegression [Error] = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,50.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,5],[1.0...|\n",
      "|       0.0|       0|[1.0,24.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(training_data)\n",
    "dt_prediction = dt_model.transform(test_data)\n",
    "\n",
    "dt_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier [Accuracy] = 0.82069\n",
      "DecisionTreeClassifier [Error] = 0.17931 \n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
    "print(\"DecisionTreeClassifier [Accuracy] = %g\"% (dt_accuracy))\n",
    "print(\"DecisionTreeClassifier [Error] = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,50.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,5],[1.0...|\n",
      "|       0.0|       0|[1.0,24.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "|       0.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(training_data)\n",
    "rf_prediction = rf_model.transform(test_data)\n",
    "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier [Accuracy] = 0.827586\n",
      "RandomForestClassifier [Error] = 0.172414\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"RandomForestClassifier [Accuracy] = %g\"% (rf_accuracy))\n",
    "print(\"RandomForestClassifier [Error] = %g\" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,50.0,0.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,5],[1.0...|\n",
      "|       0.0|       0|[1.0,24.0,0.0,0.0...|\n",
      "|       1.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "|       1.0|       0|(9,[0,1,5,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxIter=10)\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_prediction = gbt_model.transform(test_data)\n",
    "gbt_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-boosted [Accuracy] = 0.841379\n",
      "Gradient-boosted [Error] = 0.158621\n"
     ]
    }
   ],
   "source": [
    "gbt_accuracy = evaluator.evaluate(gbt_prediction)\n",
    "print(\"Gradient-boosted [Accuracy] = %g\"% (gbt_accuracy))\n",
    "print(\"Gradient-boosted [Error] = %g\"% (1.0 - gbt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.write().overwrite().save('rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "type(RandomForestClassificationModel.load('rf_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|\n",
      "|       0|     3|  male|30.0|    0|    0| 8.4583|       Q|          0|    1|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|          0|    1|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|          4|    0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|          2|    0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|          1|    0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|          2|    0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|          0|    1|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|          0|    1|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|          6|    0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|          0|    1|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|          0|    1|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|          5|    0|\n",
      "|       1|     2|  male|30.0|    0|    0|   13.0|       S|          0|    1|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|          1|    0|\n",
      "|       1|     3|female|30.0|    0|    0|  7.225|       C|          0|    1|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = titanic_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "|       0|     1|female| 2.0|    1|    2| 151.55|       S|          3|    0|\n",
      "|       0|     1|female|25.0|    1|    2| 151.55|       S|          3|    0|\n",
      "|       0|     1|female|50.0|    0|    0|28.7125|       C|          0|    1|\n",
      "|       0|     1|  male|18.0|    1|    0|  108.9|       C|          1|    0|\n",
      "|       0|     1|  male|19.0|    1|    0|   53.1|       S|          1|    0|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_sex = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_embarked = StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(\n",
    "    inputCols=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Family_Size\",\"Embarked_index\",\"Sex_index\"],\n",
    "    outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer_sex, indexer_embarked, feature, rf_classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model.write().overwrite().save('p_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load('p_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = p_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+----+-----+-----+--------+--------+-----------+-----+\n",
      "|Survived|Pclass| Sex| Age|SibSp|Parch|    Fare|Embarked|Family_Size|Alone|\n",
      "+--------+------+----+----+-----+-----+--------+--------+-----------+-----+\n",
      "|       0|     1|male|24.0|    0|    0|    79.2|       C|          0|    1|\n",
      "|       0|     1|male|24.0|    0|    1|247.5208|       C|          1|    0|\n",
      "|       0|     1|male|28.0|    0|    0|    47.1|       S|          0|    1|\n",
      "|       0|     1|male|28.0|    1|    0| 82.1708|       C|          1|    0|\n",
      "|       0|     1|male|30.0|    0|    0|     0.0|       S|          0|    1|\n",
      "+--------+------+----+----+-----+-----+--------+--------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-----+--------+-----------+--------------+---------+----------+\n",
      "|Pclass| Age|SibSp|Parch|    Fare|Family_Size|Embarked_index|Sex_index|prediction|\n",
      "+------+----+-----+-----+--------+-----------+--------------+---------+----------+\n",
      "|     1|24.0|    0|    0|    79.2|          0|           1.0|      0.0|       0.0|\n",
      "|     1|24.0|    0|    1|247.5208|          1|           1.0|      0.0|       0.0|\n",
      "|     1|28.0|    0|    0|    47.1|          0|           0.0|      0.0|       0.0|\n",
      "|     1|28.0|    1|    0| 82.1708|          1|           1.0|      0.0|       0.0|\n",
      "|     1|30.0|    0|    0|     0.0|          0|           0.0|      0.0|       0.0|\n",
      "+------+----+-----+-----+--------+-----------+--------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select([\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Family_Size\",\"Embarked_index\",\"Sex_index\",\"prediction\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Family_Size: integer (nullable = true)\n",
      " |-- Alone: integer (nullable = true)\n",
      " |-- Sex_index: double (nullable = false)\n",
      " |-- Embarked_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline model [Accuracy] = 0.834254\n",
      "Pipeline model [Error] = 0.165746 \n"
     ]
    }
   ],
   "source": [
    "p_accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Pipeline model [Accuracy] = %g\"% (p_accuracy))\n",
    "print(\"Pipeline model [Error] = %g \" % (1.0 - p_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "                  .addGrid(rf_classifier.maxDepth, [2, 3, 4])\\\n",
    "                  .addGrid(rf_classifier.maxBins, [4, 5, 6])\\\n",
    "                  .addGrid(rf_classifier.minInfoGain, [0.05, 0.1, 0.15])\\\n",
    "                  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    " tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=evaluator,\n",
    "                            trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.TrainValidationSplitModel"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_fc494b8277eb"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Num Trees: 4\n",
      "Impurity: 0.1\n"
     ]
    }
   ],
   "source": [
    "jo = model.bestModel.stages[-1]._java_obj\n",
    "print('Max Depth: {}'.format(jo.getMaxDepth()))\n",
    "print('Num Trees: {}'.format(jo.getMaxBins()))\n",
    "print('Impurity: {}'.format(jo.getMinInfoGain()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
